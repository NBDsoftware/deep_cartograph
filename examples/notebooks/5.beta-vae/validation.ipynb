{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $\\beta$ - VAE Model\n",
    "\n",
    "This is a notebook to experiment with the $\\beta$ - VAE CV. First we'll try to understand why is the $\\beta$ factor needed to control the equilibrium between regularization and reconstruction. Then, we'll try to produce a good 1D CV for biasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/Bio/Application/__init__.py:39: BiopythonDeprecationWarning: The Bio.Application modules and modules relying on it have been deprecated.\n",
      "\n",
      "Due to the on going maintenance burden of keeping command line application\n",
      "wrappers up to date, we have decided to deprecate and eventually remove these\n",
      "modules.\n",
      "\n",
      "We instead now recommend building your command line and invoking it directly\n",
      "with the subprocess module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deep_cartograph.deep_carto import deep_cartograph \n",
    "import importlib.resources as resources\n",
    "from deep_cartograph import data\n",
    "\n",
    "import logging\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Get the path to the data\n",
    "data_folder = resources.files(data)\n",
    "\n",
    "# Set logging level\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def run_deep_carto(\n",
    "    configuration: dict,\n",
    "    output_folder: str):\n",
    "    \"\"\"\n",
    "    Run the deep_cartograph workflow\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    configuration : dict\n",
    "        Configuration dictionary containing the parameters for the workflow.\n",
    "    output_folder : str\n",
    "        Path to the output folder where results will be saved.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input trajectory and topology\n",
    "    \n",
    "    input_path = f\"{data_folder}/protein_1AH7/input\"\n",
    "    traj_path = os.path.join(input_path, f'GaMD_traj.xtc')\n",
    "    top_path = os.path.join(input_path, f'topology.pdb')\n",
    "\n",
    "    ################\n",
    "    # Run workflow #\n",
    "    ################\n",
    "    deep_cartograph(\n",
    "        configuration = configuration,\n",
    "        trajectory_data = traj_path,\n",
    "        topology_data = top_path,\n",
    "        output_folder = output_folder,\n",
    "        restart = True)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Naive training with $\\beta$ = 1\n",
    "\n",
    "Here we use $\\beta$ of 1, meaning that the regularization term will be used in the loss without any dampening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deep_cartograph:================\n",
      "INFO:deep_cartograph:Analyze geometry\n",
      "INFO:deep_cartograph:================\n",
      "INFO:deep_cartograph:Elapsed time (Analyze geometry): 00 h 00 min 00 s\n",
      "INFO:deep_cartograph:================\n",
      "INFO:deep_cartograph:Compute features\n",
      "INFO:deep_cartograph:================\n",
      "INFO:MDAnalysis.core.universe:The attribute(s) types have already been read from the topology file. The guesser will only guess empty values for this attribute, if any exists. To overwrite it by completely guessed values, you can pass the attribute to the force_guess parameter instead of the to_guess one\n",
      "INFO:MDAnalysis.guesser.base:There is no empty types values. Guesser did not guess any new values for types attribute\n",
      "INFO:MDAnalysis.core.universe:attribute masses has been guessed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/MDAnalysis/coordinates/PDB.py:777: UserWarning: Unit cell dimensions not found. CRYST1 record set to unitary values.\n",
      "  warnings.warn(\"Unit cell dimensions not found. \"\n",
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/MDAnalysis/coordinates/PDB.py:1154: UserWarning: Found no information for attr: 'formalcharges' Using default value of '0'\n",
      "  warnings.warn(\"Found no information for attr: '{}'\"\n",
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/MDAnalysis/coordinates/PDB.py:1201: UserWarning: Found missing chainIDs. Corresponding atoms will use value of 'X'\n",
      "  warnings.warn(\"Found missing chainIDs.\"\n",
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/MDAnalysis/coordinates/PDB.py:453: UserWarning: 1 A^3 CRYST1 record, this is usually a placeholder. Unit cell dimensions will be set to None.\n",
      "  warnings.warn(\"1 A^3 CRYST1 record,\"\n",
      "INFO:MDAnalysis.core.universe:The attribute(s) types have already been read from the topology file. The guesser will only guess empty values for this attribute, if any exists. To overwrite it by completely guessed values, you can pass the attribute to the force_guess parameter instead of the to_guess one\n",
      "INFO:MDAnalysis.guesser.base:There is no empty types values. Guesser did not guess any new values for types attribute\n",
      "INFO:MDAnalysis.core.universe:attribute masses has been guessed successfully.\n",
      "WARNING:deep_cartograph.modules.md.md:Provided topology does not contain bonds. Bonds will be guessed using a distance criterion (bond_length < 2.0). Distances between bonded atoms will be excluded.\n",
      "INFO:MDAnalysis.core.universe:The attribute(s) types have already been read from the topology file. The guesser will only guess empty values for this attribute, if any exists. To overwrite it by completely guessed values, you can pass the attribute to the force_guess parameter instead of the to_guess one\n",
      "INFO:MDAnalysis.guesser.base:There is no empty types values. Guesser did not guess any new values for types attribute\n",
      "INFO:MDAnalysis.core.universe:attribute masses has been guessed successfully.\n",
      "WARNING:deep_cartograph.modules.md.md:Residue 1 does not have a previous residue, skipping phi dihedral.\n",
      "WARNING:deep_cartograph.modules.md.md:Residue 245 does not have a next residue, skipping psi dihedral.\n",
      "INFO:MDAnalysis.core.universe:The attribute(s) types have already been read from the topology file. The guesser will only guess empty values for this attribute, if any exists. To overwrite it by completely guessed values, you can pass the attribute to the force_guess parameter instead of the to_guess one\n",
      "INFO:MDAnalysis.guesser.base:There is no empty types values. Guesser did not guess any new values for types attribute\n",
      "INFO:MDAnalysis.core.universe:attribute masses has been guessed successfully.\n",
      "INFO:deep_cartograph:Computing features for GaMD_traj with topology topology...\n",
      "INFO:deep_cartograph:Skipping topology. Colvars file already exists.\n",
      "INFO:deep_cartograph:Elapsed time (Compute features): 00 h 00 min 00 s\n",
      "INFO:deep_cartograph:==================\n",
      "INFO:deep_cartograph:Filtering features\n",
      "INFO:deep_cartograph:==================\n",
      "INFO:deep_cartograph:Finding the features that contains the most information about the transitions or conformational changes.\n",
      "INFO:deep_cartograph:The following algorithms are available:\n",
      "INFO:deep_cartograph:- Hartigan's dip test filter. Keeps features that are not unimodal.\n",
      "INFO:deep_cartograph:- Shannon entropy filter. Keeps features with entropy greater than a threshold.\n",
      "INFO:deep_cartograph:- Standard deviation filter. Keeps features with standard deviation greater than a threshold.\n",
      "INFO:deep_cartograph:Note that the all features must be in the same units to apply the entropy and standard deviation filters meaningfully.\n",
      "INFO:deep_cartograph:Filtered features file already exists: /home/pnavarro/repos/NBDsoftware/deep_cartograph/deep_cartograph/data/protein_1AH7/output/filter_features/filtered_features.txt. Skipping filtering.\n",
      "INFO:deep_cartograph.modules.common.common: Using features in /home/pnavarro/repos/NBDsoftware/deep_cartograph/deep_cartograph/data/protein_1AH7/output/filter_features/filtered_features.txt\n",
      "INFO:deep_cartograph:================================\n",
      "INFO:deep_cartograph:Training of Collective Variables\n",
      "INFO:deep_cartograph:================================\n",
      "INFO:deep_cartograph:Training of collective variables using the mlcolvar library.\n",
      "INFO:deep_cartograph.tools.train_colvars.train_colvars_workflow:Collective variables to compute: ['vae']\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Reading training data from colvars files...\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Number of features: 97\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Number of samples: 4000\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Calculating VAE ...\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Training VAE ...\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Model architecture: VariationalAutoEncoderCV(\n",
      "  (loss_fn): ELBOGaussiansLoss()\n",
      "  (norm_in): Normalization(in_features=97, out_features=97, mode=mean_std)\n",
      "  (encoder): FeedForward(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=97, out_features=32, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "      (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (mean_nn): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (log_var_nn): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (decoder): FeedForward(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=8, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "      (3): Linear(in_features=8, out_features=16, bias=True)\n",
      "      (4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "      (6): Linear(in_features=16, out_features=97, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (8): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/pnavarro/.conda/envs/deep_cartograph/lib/p ...\n",
      "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /shared/scratch/jobs/pnavarro/checkpoints exists and is not empty.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name       | Type              | Params | Mode  | In sizes | Out sizes\n",
      "--------------------------------------------------------------------------------\n",
      "0 | loss_fn    | ELBOGaussiansLoss | 0      | train | ?        | ?        \n",
      "1 | norm_in    | Normalization     | 0      | train | [1, 97]  | [1, 97]  \n",
      "2 | encoder    | FeedForward       | 3.7 K  | train | [1, 97]  | [1, 16]  \n",
      "3 | mean_nn    | Linear            | 34     | train | [1, 16]  | [1, 2]   \n",
      "4 | log_var_nn | Linear            | 34     | train | ?        | ?        \n",
      "5 | decoder    | FeedForward       | 1.8 K  | train | ?        | ?        \n",
      "--------------------------------------------------------------------------------\n",
      "5.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 K     Total params\n",
      "0.022     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name       | Type              | Params | Mode  | In sizes | Out sizes\n",
      "--------------------------------------------------------------------------------\n",
      "0 | loss_fn    | ELBOGaussiansLoss | 0      | train | ?        | ?        \n",
      "1 | norm_in    | Normalization     | 0      | train | [1, 97]  | [1, 97]  \n",
      "2 | encoder    | FeedForward       | 3.7 K  | train | [1, 97]  | [1, 16]  \n",
      "3 | mean_nn    | Linear            | 34     | train | [1, 16]  | [1, 2]   \n",
      "4 | log_var_nn | Linear            | 34     | train | ?        | ?        \n",
      "5 | decoder    | FeedForward       | 1.8 K  | train | ?        | ?        \n",
      "--------------------------------------------------------------------------------\n",
      "5.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 K     Total params\n",
      "0.022     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO: \n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "INFO:lightning.pytorch.utilities.rank_zero:\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "ERROR:deep_cartograph.tools.train_colvars.cv_calculator:VAE training failed. Error message: name 'exit' is not defined\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Retrying VAE training...\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Model architecture: VariationalAutoEncoderCV(\n",
      "  (loss_fn): ELBOGaussiansLoss()\n",
      "  (norm_in): Normalization(in_features=97, out_features=97, mode=mean_std)\n",
      "  (encoder): FeedForward(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=97, out_features=32, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "      (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (mean_nn): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (log_var_nn): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (decoder): FeedForward(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=8, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "      (3): Linear(in_features=8, out_features=16, bias=True)\n",
      "      (4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "      (6): Linear(in_features=16, out_features=97, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (8): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name       | Type              | Params | Mode  | In sizes | Out sizes\n",
      "--------------------------------------------------------------------------------\n",
      "0 | loss_fn    | ELBOGaussiansLoss | 0      | train | ?        | ?        \n",
      "1 | norm_in    | Normalization     | 0      | train | [1, 97]  | [1, 97]  \n",
      "2 | encoder    | FeedForward       | 3.7 K  | train | [1, 97]  | [1, 16]  \n",
      "3 | mean_nn    | Linear            | 34     | train | [1, 16]  | [1, 2]   \n",
      "4 | log_var_nn | Linear            | 34     | train | ?        | ?        \n",
      "5 | decoder    | FeedForward       | 1.8 K  | train | ?        | ?        \n",
      "--------------------------------------------------------------------------------\n",
      "5.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 K     Total params\n",
      "0.022     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name       | Type              | Params | Mode  | In sizes | Out sizes\n",
      "--------------------------------------------------------------------------------\n",
      "0 | loss_fn    | ELBOGaussiansLoss | 0      | train | ?        | ?        \n",
      "1 | norm_in    | Normalization     | 0      | train | [1, 97]  | [1, 97]  \n",
      "2 | encoder    | FeedForward       | 3.7 K  | train | [1, 97]  | [1, 16]  \n",
      "3 | mean_nn    | Linear            | 34     | train | [1, 16]  | [1, 2]   \n",
      "4 | log_var_nn | Linear            | 34     | train | ?        | ?        \n",
      "5 | decoder    | FeedForward       | 1.8 K  | train | ?        | ?        \n",
      "--------------------------------------------------------------------------------\n",
      "5.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 K     Total params\n",
      "0.022     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "INFO:lightning.pytorch.utilities.rank_zero:\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "ERROR:deep_cartograph.tools.train_colvars.cv_calculator:VAE training failed. Error message: name 'exit' is not defined\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Retrying VAE training...\n",
      "INFO:deep_cartograph.tools.train_colvars.cv_calculator:Model architecture: VariationalAutoEncoderCV(\n",
      "  (loss_fn): ELBOGaussiansLoss()\n",
      "  (norm_in): Normalization(in_features=97, out_features=97, mode=mean_std)\n",
      "  (encoder): FeedForward(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=97, out_features=32, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "      (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (mean_nn): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (log_var_nn): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (decoder): FeedForward(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=8, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "      (3): Linear(in_features=8, out_features=16, bias=True)\n",
      "      (4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "      (6): Linear(in_features=16, out_features=97, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (8): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name       | Type              | Params | Mode  | In sizes | Out sizes\n",
      "--------------------------------------------------------------------------------\n",
      "0 | loss_fn    | ELBOGaussiansLoss | 0      | train | ?        | ?        \n",
      "1 | norm_in    | Normalization     | 0      | train | [1, 97]  | [1, 97]  \n",
      "2 | encoder    | FeedForward       | 3.7 K  | train | [1, 97]  | [1, 16]  \n",
      "3 | mean_nn    | Linear            | 34     | train | [1, 16]  | [1, 2]   \n",
      "4 | log_var_nn | Linear            | 34     | train | ?        | ?        \n",
      "5 | decoder    | FeedForward       | 1.8 K  | train | ?        | ?        \n",
      "--------------------------------------------------------------------------------\n",
      "5.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 K     Total params\n",
      "0.022     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name       | Type              | Params | Mode  | In sizes | Out sizes\n",
      "--------------------------------------------------------------------------------\n",
      "0 | loss_fn    | ELBOGaussiansLoss | 0      | train | ?        | ?        \n",
      "1 | norm_in    | Normalization     | 0      | train | [1, 97]  | [1, 97]  \n",
      "2 | encoder    | FeedForward       | 3.7 K  | train | [1, 97]  | [1, 16]  \n",
      "3 | mean_nn    | Linear            | 34     | train | [1, 16]  | [1, 2]   \n",
      "4 | log_var_nn | Linear            | 34     | train | ?        | ?        \n",
      "5 | decoder    | FeedForward       | 1.8 K  | train | ?        | ?        \n",
      "--------------------------------------------------------------------------------\n",
      "5.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 K     Total params\n",
      "0.022     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO: [rank: 0] Received SIGTERM: 15\n",
      "INFO:lightning.pytorch.trainer.connectors.signal_connector:[rank: 0] Received SIGTERM: 15\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Input configuration\n",
    "config_path = f\"{data_folder}/protein_1AH7/config.yml\"\n",
    "\n",
    "with open(config_path) as config_file:\n",
    "    configuration = yaml.load(config_file, Loader = yaml.FullLoader)\n",
    "    \n",
    "# Output folder\n",
    "output_folder = f\"{data_folder}/protein_1AH7/output_1\"\n",
    "\n",
    "# Modify annealing parameters\n",
    "kl_annealing_args = {\n",
    "    'type': 'linear',\n",
    "    'start_beta': 1.0,\n",
    "    'max_beta': 1.0,\n",
    "}\n",
    "configuration['train_colvars']['common']['training']['kl_annealing'] = kl_annealing_args\n",
    "run_deep_carto(configuration, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe what is known as posterior collapse or KL vanishing. The model finds a way to reduce the loss without learning any useful representation, just by reducing the KL divergence between the posterior distribution (learned by the encoder) and the gaussian with mean 0 and variance one. The latent space loses its ability to represent any information specific to the input data. The encoder effectively ignores the input x and just outputs the prior. This is why we see all data points clustered around (0,0) â€“ the model has found a \"lazy\" way to satisfy the KL constraint without actually encoding any useful information.\n",
    "\n",
    "This is a common situation when training VAE and is affected by the following factors:\n",
    "\n",
    "- Too high $\\beta$: the regularization term has too much weight early on and the encoder stays trapped learning an uninformative distribution.\n",
    "- Overly Powerful/Flexible Decoder: If your decoder is too strong or has too much capacity it can learn to reconstruct the input data x even if the latent code z contains little to no information.\n",
    "- Learning Rate to high or too low\n",
    "- Simple or small datasets\n",
    "- Large Batch Sizes\n",
    "- Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Linear annealing from $\\beta$ = 0 to $\\beta$ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input configuration\n",
    "config_path = f\"{data_folder}/protein_1AH7/config.yml\"\n",
    "\n",
    "with open(config_path) as config_file:\n",
    "    configuration = yaml.load(config_file, Loader = yaml.FullLoader)\n",
    "    \n",
    "# Output folder\n",
    "output_folder = f\"{data_folder}/protein_1AH7/output_2\"\n",
    "\n",
    "# Modify annealing parameters\n",
    "kl_annealing_args = {\n",
    "    'type': 'linear',\n",
    "    'start_beta': 0.0,\n",
    "    'max_beta': 0.001,\n",
    "    'start_epoch': 1000,\n",
    "    'n_epochs_anneal': 5000\n",
    "}\n",
    "configuration['train_colvars']['common']['training']['kl_annealing'] = kl_annealing_args\n",
    "run_deep_carto(configuration, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Linear annealing from $\\beta$ = 0.00001 to $\\beta$ = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Cyclical annealing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
