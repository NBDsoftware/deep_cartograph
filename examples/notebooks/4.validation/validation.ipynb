{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Validation\n",
    "\n",
    "Validate results using Deep Carto package against results using mlcolvars\n",
    "\n",
    "This notebook can be used for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_cartograph.modules.plumed.utils import read_as_pandas\n",
    "from deep_cartograph.run import deep_cartograph \n",
    "import importlib.resources as resources\n",
    "from deep_cartograph import data\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from IPython.display import display, HTML\n",
    "from typing import Tuple, Dict, List, Literal\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import shutil\n",
    "import yaml\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Get the path to the data\n",
    "data_folder = resources.files(data)\n",
    "\n",
    "# Set logging level\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def run_new_deep_carto(features: Literal['torsions', 'distances'], system_name: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    \n",
    "    Run the deep_cartograph workflow for a given system and feature set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    features : Literal['torsions', 'distances']\n",
    "        The feature set to use.\n",
    "    \n",
    "    system_name : str\n",
    "        The name of the system to analyze.\n",
    "        \n",
    "    output_folder : str\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input trajectory and topology\n",
    "    input_path = f\"{data_folder}/calpha_transitions/input/{system_name}\"\n",
    "    traj_path = os.path.join(input_path, f'{system_name}.dcd')\n",
    "    top_path = os.path.join(input_path, f'{system_name}.pdb')\n",
    "\n",
    "    # Input configuration\n",
    "    config_path = f\"{data_folder}/calpha_transitions/input/{features}_config_validation.yml\"\n",
    "    with open(config_path) as config_file:\n",
    "        configuration = yaml.load(config_file, Loader = yaml.FullLoader)\n",
    "\n",
    "    # Clean output folder\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "\n",
    "    ################\n",
    "    # Run workflow #\n",
    "    ################\n",
    "    deep_cartograph(\n",
    "        configuration = configuration,\n",
    "        trajectory_data = traj_path,\n",
    "        topology_data = top_path,\n",
    "        output_folder = output_folder)\n",
    "\n",
    "    return\n",
    "\n",
    "def run_old_deep_carto(features: Literal['torsions', 'distances'], system_name: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    \n",
    "    Run the deep_cartograph workflow for a given system and feature set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    features : Literal['torsions', 'distances']\n",
    "        The feature set to use.\n",
    "    \n",
    "    system_name : str\n",
    "        The name of the system to analyze.\n",
    "        \n",
    "    output_folder : str\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input trajectory and topology\n",
    "    input_path = f\"{data_folder}/calpha_transitions/input/{system_name}\"\n",
    "    traj_path = os.path.join(input_path, f'{system_name}.dcd')\n",
    "    top_path = os.path.join(input_path, f'{system_name}.pdb')\n",
    "\n",
    "    # Input configuration\n",
    "    config_path = f\"{data_folder}/calpha_transitions/input/{features}_config_validation.yml\"\n",
    "    with open(config_path) as config_file:\n",
    "        configuration = yaml.load(config_file, Loader = yaml.FullLoader)\n",
    "\n",
    "    # Clean output folder\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "\n",
    "    ################\n",
    "    # Run workflow #\n",
    "    ################\n",
    "    deep_cartograph(\n",
    "        configuration = configuration,\n",
    "        trajectory_data = traj_path,\n",
    "        topology_data = top_path,\n",
    "        output_folder = output_folder)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New results\n",
    "\n",
    "For distance and torsion angle features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_name = '1rcs_B-3ssx_R-3'\n",
    "features = 'distances'\n",
    "output_path = f\"output_new/{system_name}/{features}\"\n",
    "\n",
    "run_new_deep_carto(features, system_name, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 'torsions'\n",
    "output_path = f\"output_new/{system_name}/{features}\"\n",
    "\n",
    "run_new_deep_carto(features, system_name, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for mlcolvars\n",
    "\n",
    "Here we load the same colvars files and we use the mlcolvars package directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_cartograph.modules.common import read_feature_constraints\n",
    "from deep_cartograph.modules.figures import figures\n",
    "\n",
    "from mlcolvar.utils.timelagged import create_timelagged_dataset\n",
    "from mlcolvar.utils.io import create_dataset_from_files\n",
    "from mlcolvar.utils.trainer import MetricsCallback\n",
    "from mlcolvar.utils.plot import plot_metrics\n",
    "from mlcolvar.core.transform.utils import Statistics\n",
    "from mlcolvar.core.transform import Normalization\n",
    "from mlcolvar.data import DictModule, DictDataset\n",
    "from mlcolvar.cvs import AutoEncoderCV, DeepTICA\n",
    "from mlcolvar.core.stats import PCA, TICA\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "import lightning \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deep_cartograph.modules.common.common: Using features in output_new/1rcs_B-3ssx_R-3/distances/filter_features/filtered_features.txt\n",
      "/home/pnavarro/repos/mlcolvar/mlcolvar/utils/timelagged.py:140: UserWarning: Monitoring the progress for the search of time-lagged configurations with a progress_bar requires `tqdm`.\n",
      "  warnings.warn(\n",
      "/home/pnavarro/repos/mlcolvar/mlcolvar/utils/timelagged.py:186: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  x_t = torch.stack(x_t) if type(x) == torch.Tensor else torch.Tensor(x_t)\n"
     ]
    }
   ],
   "source": [
    "# Path to output\n",
    "parent_output_path = 'output_mlcolvars'\n",
    "\n",
    "features='distances' \n",
    "\n",
    "# Path to features\n",
    "colvars_path = f\"output_new/{system_name}/{features}/compute_features/{system_name}/colvars.dat\"\n",
    "\n",
    "# Find filtered features\n",
    "filtered_features_path = f\"output_new/{system_name}/{features}/filter_features/filtered_features.txt\"\n",
    "feature_list = read_feature_constraints(filtered_features_path)\n",
    "feature_filter = dict(items=feature_list)\n",
    "\n",
    "# Create training dataset\n",
    "training_input_dtset = create_dataset_from_files(file_names=colvars_path, filter_args=feature_filter, verbose=False, return_dataframe=False)\n",
    "\n",
    "# Find number of features\n",
    "num_features = training_input_dtset[\"data\"].shape[1]\n",
    "        \n",
    "# Compute normalization layer using training data statistics\n",
    "training_data_stats = Statistics(training_input_dtset[:]['data']).to_dict()\n",
    "features_normalization = Normalization(num_features, mean=training_data_stats[\"mean\"], range=training_data_stats[\"std\"])\n",
    "features_mean = training_data_stats[\"mean\"]\n",
    "features_std = training_data_stats[\"std\"]\n",
    "\n",
    "# Normalize data\n",
    "normalized_training_data = features_normalization(training_input_dtset[:]['data'])\n",
    "\n",
    "# Create time-lagged dataset \n",
    "normalized_lagged_training_data = create_timelagged_dataset(normalized_training_data.numpy(), lag_time=1)\n",
    "\n",
    "# Save mean and std \n",
    "np.savetxt(os.path.join(parent_output_path, system_name, features, 'features_mean.txt'), features_mean)\n",
    "np.savetxt(os.path.join(parent_output_path, system_name, features, 'features_std.txt'), features_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(parent_output_path, system_name, features, 'pca')\n",
    "\n",
    "# Create output folder\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Compute PCA\n",
    "pca_obj = PCA(in_features=num_features)\n",
    "pca_eigvals, pca_eigvecs = pca_obj.compute(X=torch.tensor(normalized_training_data.numpy()), center = True)\n",
    "pca_cv =  pca_eigvecs[:,0:2].numpy()\n",
    "np.savetxt(os.path.join(output_path, f'weights.txt'), pca_cv)\n",
    "\n",
    "# Project the training data\n",
    "projected_training_data = np.matmul(normalized_training_data.numpy(), pca_cv)\n",
    "\n",
    "# Compute statistics of the projected training data\n",
    "stats = Statistics(torch.tensor(projected_training_data))\n",
    "\n",
    "# Find the normalization for the CV space\n",
    "cv_normalization = Normalization(2, mode='min_max', stats = stats)\n",
    "\n",
    "# Normalize the projected training data in the CV space\n",
    "normalized_projected_training_data = cv_normalization(torch.tensor(projected_training_data)).numpy()\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "projected_colvars_df = pd.DataFrame(normalized_projected_training_data, columns=['PC 1', 'PC 2'])\n",
    "\n",
    "# Add a column with the order of the data points\n",
    "projected_colvars_df['order'] = np.arange(projected_colvars_df.shape[0])\n",
    "\n",
    "figure_settings = {\n",
    "    'plot': True,\n",
    "    'num_bins': 100,\n",
    "    'bandwidth': 0.25,\n",
    "    'alpha': 0.6,\n",
    "    'cmap': 'turbo',\n",
    "    'use_legend': True,\n",
    "    'marker_size': 12,\n",
    "}\n",
    "\n",
    "figures.gradient_scatter_plot(\n",
    "    data = projected_colvars_df,\n",
    "    column_labels = ['PC 1', 'PC 2'],\n",
    "    color_label = 'order',\n",
    "    settings = figure_settings,\n",
    "    file_path = os.path.join(output_path,'trajectory.png'))\n",
    "                        \n",
    "# Save the max and min values for each dimension\n",
    "stats_dict = stats.to_dict()\n",
    "np.savetxt(os.path.join(output_path, 'cv_max.txt'), stats_dict['max'])\n",
    "np.savetxt(os.path.join(output_path, 'cv_min.txt'), stats_dict['min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnavarro/repos/mlcolvar/mlcolvar/core/stats/utils.py:224: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  ave = torch.mean(x.T, 1, keepdim=False).T\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(parent_output_path, system_name, features, 'tica')\n",
    "\n",
    "# Create output folder\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "# Compute TICA\n",
    "tica_obj = TICA(in_features = num_features, out_features=2)\n",
    "tica_eigvals, tica_eigvecs = tica_obj.compute(data=[normalized_lagged_training_data['data'], normalized_lagged_training_data['data_lag']], save_params = True, remove_average = True)\n",
    "tica_cv = tica_eigvecs.numpy() \n",
    "np.savetxt(os.path.join(output_path, f'weights.txt'), tica_cv)\n",
    "\n",
    "# Project the training data\n",
    "projected_training_data = np.matmul(normalized_training_data.numpy(), tica_cv)\n",
    "\n",
    "# Compute statistics of the projected training data\n",
    "stats = Statistics(torch.tensor(projected_training_data))\n",
    "\n",
    "# Find the normalization for the CV space\n",
    "cv_normalization = Normalization(2, mode='min_max', stats = stats)\n",
    "\n",
    "# Normalize the projected training data in the CV space\n",
    "normalized_projected_training_data = cv_normalization(torch.tensor(projected_training_data)).numpy()\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "projected_colvars_df = pd.DataFrame(normalized_projected_training_data, columns=['TIC 1', 'TIC 2'])\n",
    "\n",
    "# Add a column with the order of the data points\n",
    "projected_colvars_df['order'] = np.arange(projected_colvars_df.shape[0])\n",
    "\n",
    "figure_settings = {\n",
    "    'plot': True,\n",
    "    'num_bins': 100,\n",
    "    'bandwidth': 0.25,\n",
    "    'alpha': 0.6,\n",
    "    'cmap': 'turbo',\n",
    "    'use_legend': True,\n",
    "    'marker_size': 12,\n",
    "}\n",
    "\n",
    "figures.gradient_scatter_plot(\n",
    "    data = projected_colvars_df,\n",
    "    column_labels = ['TIC 1', 'TIC 2'],\n",
    "    color_label = 'order',\n",
    "    settings = figure_settings,\n",
    "    file_path = os.path.join(output_path,'trajectory.png'))\n",
    "                        \n",
    "# Save the max and min values for each dimension\n",
    "stats_dict = stats.to_dict()\n",
    "np.savetxt(os.path.join(output_path, 'cv_max.txt'), stats_dict['max'])\n",
    "np.savetxt(os.path.join(output_path, 'cv_min.txt'), stats_dict['min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/pnavarro/.conda/envs/deep_cartograph/lib/pytho ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /home/pnavarro/repos/NBDsoftware/deep_cartograph/examples/notebooks/4.validation/output_mlcolvars/1rcs_B-3ssx_R-3/distances/ae exists and is not empty.\n",
      "\n",
      "  | Name    | Type          | Params | Mode  | In sizes | Out sizes\n",
      "-------------------------------------------------------------------------\n",
      "0 | loss_fn | MSELoss       | 0      | train | ?        | ?        \n",
      "1 | norm_in | Normalization | 0      | train | [1, 41]  | [1, 41]  \n",
      "2 | encoder | FeedForward   | 236    | train | [1, 41]  | [1, 2]   \n",
      "3 | decoder | FeedForward   | 275    | train | ?        | ?        \n",
      "-------------------------------------------------------------------------\n",
      "511       Trainable params\n",
      "0         Non-trainable params\n",
      "511       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score: 0.006817468907684088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_path = os.path.join(parent_output_path, system_name, features, 'ae')\n",
    "\n",
    "# Create output folder\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "# Create datamodule for trainings\n",
    "datamodule = DictModule(\n",
    "    random_split = True,\n",
    "    dataset = training_input_dtset,\n",
    "    lengths = [0.8, 0.2],\n",
    "    batch_size = 128,\n",
    "    shuffle = False, \n",
    "    generator = torch.manual_seed(42))\n",
    "\n",
    "# Define layers\n",
    "hidden_layers = [5, 3]\n",
    "nn_layers =  [num_features] + hidden_layers + [2]\n",
    "\n",
    "# Define options\n",
    "options = {\n",
    "    \"norm_in\" : {'mode' : 'mean_std'},\n",
    "    \"encoder\": {'activation': 'shifted_softplus', 'dropout': 0.1},\n",
    "    \"decoder\": {'activation': 'shifted_softplus', 'dropout': 0.1},\n",
    "    \"optimizer\": {'lr': 1e-3, 'weight_decay': 0}\n",
    "}\n",
    "\n",
    "# Compute Autoencoder\n",
    "ae = AutoEncoderCV(nn_layers,  options=options)\n",
    "\n",
    "# Set optimizer\n",
    "ae._optimizer_name = 'Adam'\n",
    "\n",
    "# Define MetricsCallback to store the loss\n",
    "metrics = MetricsCallback()\n",
    "\n",
    "# Define EarlyStopping callback to stop training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"valid_loss\", \n",
    "    min_delta=1.0e-05, \n",
    "    patience=100, \n",
    "    mode = \"min\")\n",
    "\n",
    "# Define ModelCheckpoint callback to save the best model\n",
    "checkpoint = ModelCheckpoint(\n",
    "    dirpath=output_path,\n",
    "    monitor=\"valid_loss\",                    \n",
    "    save_last=False,                          \n",
    "    save_top_k=1,                             \n",
    "    save_weights_only=True,                   \n",
    "    filename=None,                            \n",
    "    mode=\"min\",                                \n",
    "    every_n_epochs=1)\n",
    "\n",
    "# Define trainer\n",
    "trainer = lightning.Trainer(          \n",
    "    callbacks=[metrics, early_stopping, checkpoint],\n",
    "    max_epochs=10000, \n",
    "    logger=False, \n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar = False, \n",
    "    check_val_every_n_epoch=1)\n",
    "\n",
    "trainer.fit(ae, datamodule)\n",
    "\n",
    "# Load the best model\n",
    "best_model = AutoEncoderCV.load_from_checkpoint(checkpoint.best_model_path)\n",
    "\n",
    "# Find the score\n",
    "best_model_score = checkpoint.best_model_score\n",
    "print(f'Best model score: {best_model_score}')\n",
    "\n",
    "ax = plot_metrics(metrics.metrics, \n",
    "                    labels=['Training', 'Validation'], \n",
    "                    keys=['train_loss', 'valid_loss'], \n",
    "                    linestyles=['-','-'], colors=['fessa1','fessa5'], \n",
    "                    yscale='log')            \n",
    "# Save figure\n",
    "ax.figure.savefig(os.path.join(output_path, f'loss.png'), dpi=300, bbox_inches='tight')\n",
    "ax.figure.clf()\n",
    "\n",
    "# After training, put model in evaluation mode - needed for cv normalization and data projection\n",
    "best_model.eval()\n",
    "\n",
    "# Data projected onto original latent space of the best model - feature normalization included in the model\n",
    "with torch.no_grad():\n",
    "    best_model.postprocessing = None\n",
    "    projected_training_data = best_model(training_input_dtset[:]['data'])\n",
    "    \n",
    "# Compute statistics of the projected training data\n",
    "stats = Statistics(projected_training_data)\n",
    "\n",
    "# Find the normalization for the CV space\n",
    "cv_normalization = Normalization(2, mode='min_max', stats = stats)\n",
    "\n",
    "# Set the normalization postprocessing layer of the model\n",
    "best_model.postprocessing = cv_normalization\n",
    "\n",
    "# Data projected onto normalized latent space - feature and latent space normalization included in the model\n",
    "with torch.no_grad():\n",
    "    normalized_projected_training_data = best_model(training_input_dtset[:]['data']).numpy()\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "projected_colvars_df = pd.DataFrame(normalized_projected_training_data, columns=['AE 1', 'AE 2'])\n",
    "\n",
    "# Add a column with the order of the data points\n",
    "projected_colvars_df['order'] = np.arange(projected_colvars_df.shape[0])\n",
    "\n",
    "figure_settings = {\n",
    "    'plot': True,\n",
    "    'num_bins': 100,\n",
    "    'bandwidth': 0.25,\n",
    "    'alpha': 0.6,\n",
    "    'cmap': 'turbo',\n",
    "    'use_legend': True,\n",
    "    'marker_size': 12,\n",
    "}\n",
    "\n",
    "figures.gradient_scatter_plot(\n",
    "    data = projected_colvars_df,\n",
    "    column_labels = ['AE 1', 'AE 2'],\n",
    "    color_label = 'order',\n",
    "    settings = figure_settings,\n",
    "    file_path = os.path.join(output_path,'trajectory.png'))\n",
    "\n",
    "# The projection should be min max normalized, check it here\n",
    "projected_data_stats = Statistics(torch.tensor(normalized_projected_training_data))\n",
    "projected_data_stats_dict = projected_data_stats.to_dict()\n",
    "np.savetxt(os.path.join(output_path, 'cv_max.txt'), projected_data_stats_dict['max'])\n",
    "np.savetxt(os.path.join(output_path, 'cv_min.txt'), projected_data_stats_dict['min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep TICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnavarro/repos/mlcolvar/mlcolvar/utils/timelagged.py:140: UserWarning: Monitoring the progress for the search of time-lagged configurations with a progress_bar requires `tqdm`.\n",
      "  warnings.warn(\n",
      "/home/pnavarro/.conda/envs/deep_cartograph/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/pnavarro/.conda/envs/deep_cartograph/lib/pytho ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type                  | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------------------\n",
      "0 | loss_fn | ReduceEigenvaluesLoss | 0      | train | ?        | ?        \n",
      "1 | norm_in | Normalization         | 0      | train | [1, 41]  | [1, 41]  \n",
      "2 | nn      | FeedForward           | 236    | train | [1, 41]  | [1, 2]   \n",
      "3 | tica    | TICA                  | 0      | train | [1, 2]   | [1, 2]   \n",
      "---------------------------------------------------------------------------------\n",
      "236       Trainable params\n",
      "0         Non-trainable params\n",
      "236       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score: -1.968808650970459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnavarro/repos/mlcolvar/mlcolvar/utils/plot.py:254: UserWarning: Data has no positive values, and therefore cannot be log-scaled.\n",
      "  ax.set_yscale(yscale)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalue 1: 1.0133341550827026\n",
      "Eigenvalue 2: 0.970547616481781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_path = os.path.join(parent_output_path, system_name, features, 'deep_tica')\n",
    "\n",
    "# Create time-lagged dataset (composed by pairs of samples at time t, t+lag)\n",
    "lagged_training_input_dtset = create_timelagged_dataset(training_input_dtset[:]['data'].numpy(), lag_time=1)\n",
    "        \n",
    "# Create output folder\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "# Create datamodule for trainings\n",
    "datamodule = DictModule(\n",
    "    random_split = True,\n",
    "    dataset = lagged_training_input_dtset,\n",
    "    lengths = [0.8, 0.2],\n",
    "    batch_size = 128,\n",
    "    shuffle = False, \n",
    "    generator = torch.manual_seed(42))\n",
    "\n",
    "# Define layers\n",
    "nn_layers =  [num_features] + hidden_layers + [2]\n",
    "\n",
    "# Define options\n",
    "options = {\n",
    "    \"norm_in\" : {'mode' : 'mean_std'},\n",
    "    \"nn\": {'activation': 'shifted_softplus', 'dropout': 0.1},\n",
    "    \"optimizer\": {'lr': 1e-3, 'weight_decay': 0}\n",
    "}\n",
    "\n",
    "dtica = DeepTICA(nn_layers,  options=options)\n",
    "\n",
    "# Set optimizer\n",
    "dtica._optimizer_name = 'Adam'\n",
    "\n",
    "# Define MetricsCallback to store the loss\n",
    "metrics = MetricsCallback()\n",
    "\n",
    "# Define EarlyStopping callback to stop training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"valid_loss\", \n",
    "    min_delta=1.0e-05, \n",
    "    patience=100, \n",
    "    mode = \"min\")\n",
    "\n",
    "# Define ModelCheckpoint callback to save the best model\n",
    "checkpoint = ModelCheckpoint(\n",
    "    dirpath=output_path,\n",
    "    monitor=\"valid_loss\",                    \n",
    "    save_last=False,                          \n",
    "    save_top_k=1,                             \n",
    "    save_weights_only=True,                   \n",
    "    filename=None,                            \n",
    "    mode=\"min\",                                \n",
    "    every_n_epochs=1)\n",
    "\n",
    "# Define trainer\n",
    "trainer = lightning.Trainer(          \n",
    "    callbacks=[metrics, early_stopping, checkpoint],\n",
    "    max_epochs=10000, \n",
    "    logger=False, \n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar = False, \n",
    "    check_val_every_n_epoch=1)\n",
    "\n",
    "trainer.fit(dtica, datamodule)\n",
    "\n",
    "# Load the best model\n",
    "best_model = DeepTICA.load_from_checkpoint(checkpoint.best_model_path)\n",
    "\n",
    "# Find the score\n",
    "best_model_score = checkpoint.best_model_score\n",
    "print(f'Best model score: {best_model_score}')\n",
    "\n",
    "ax = plot_metrics(metrics.metrics,\n",
    "                    labels=['Training', 'Validation'], \n",
    "                    keys=['train_loss', 'valid_loss'], \n",
    "                    linestyles=['-','-'], colors=['fessa1','fessa5'], \n",
    "                    yscale='log')\n",
    "\n",
    "# Save figure\n",
    "ax.figure.savefig(os.path.join(output_path, f'loss.png'), dpi=300, bbox_inches='tight')\n",
    "ax.figure.clf()\n",
    "\n",
    "# After training, put model in evaluation mode - needed for cv normalization and data projection\n",
    "best_model.eval()\n",
    "\n",
    "# Data projected onto original latent space of the best model - feature normalization included in the model\n",
    "with torch.no_grad():\n",
    "    best_model.postprocessing = None\n",
    "    projected_training_data = best_model(lagged_training_input_dtset[:]['data'])\n",
    "    \n",
    "# Compute statistics of the projected training data\n",
    "stats = Statistics(projected_training_data)\n",
    "\n",
    "# Find the normalization for the CV space\n",
    "cv_normalization = Normalization(2, mode='min_max', stats = stats)\n",
    "\n",
    "# Set the normalization postprocessing layer of the model\n",
    "best_model.postprocessing = cv_normalization\n",
    "\n",
    "# Find the epoch where the best model was found\n",
    "best_index = metrics.metrics['valid_loss'].index(best_model_score)\n",
    "best_epoch = metrics.metrics['epoch'][best_index]\n",
    "\n",
    "# Find eigenvalues of the best model\n",
    "best_eigvals = [metrics.metrics[f'valid_eigval_{i+1}'][best_index] for i in range(2)]\n",
    "for i in range(2):\n",
    "    print(f'Eigenvalue {i+1}: {best_eigvals[i]}')\n",
    "\n",
    "np.savetxt(os.path.join(output_path, 'eigenvalues.txt'), np.array(best_eigvals))\n",
    "\n",
    "# Plot eigenvalues\n",
    "ax = plot_metrics(metrics.metrics,\n",
    "                    labels=[f'Eigenvalue {i+1}' for i in range(2)], \n",
    "                    keys=[f'valid_eigval_{i+1}' for i in range(2)],\n",
    "                    ylabel='Eigenvalue',\n",
    "                    yscale=None)\n",
    "\n",
    "# Save figure\n",
    "ax.figure.savefig(os.path.join(output_path, f'eigenvalues.png'), dpi=300, bbox_inches='tight')\n",
    "ax.figure.clf()\n",
    "\n",
    "# Data projected onto normalized latent space - feature and latent space normalization included in the model\n",
    "with torch.no_grad():\n",
    "    normalized_projected_training_data = best_model(lagged_training_input_dtset[:]['data']).numpy()\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "projected_colvars_df = pd.DataFrame(normalized_projected_training_data, columns=['Deep TIC 1', 'Deep TIC 2'])\n",
    "\n",
    "# Add a column with the order of the data points\n",
    "projected_colvars_df['order'] = np.arange(projected_colvars_df.shape[0])\n",
    "\n",
    "figure_settings = {\n",
    "    'plot': True,\n",
    "    'num_bins': 100,\n",
    "    'bandwidth': 0.25,\n",
    "    'alpha': 0.6,\n",
    "    'cmap': 'turbo',\n",
    "    'use_legend': True,\n",
    "    'marker_size': 12,\n",
    "}\n",
    "\n",
    "figures.gradient_scatter_plot(\n",
    "    data = projected_colvars_df,\n",
    "    column_labels = ['Deep TIC 1', 'Deep TIC 2'],\n",
    "    color_label = 'order',\n",
    "    settings = figure_settings,\n",
    "    file_path = os.path.join(output_path,'trajectory.png'))\n",
    "\n",
    "# The projection should be min max normalized, check it here\n",
    "projected_data_stats = Statistics(torch.tensor(normalized_projected_training_data))\n",
    "projected_data_stats_dict = projected_data_stats.to_dict()\n",
    "np.savetxt(os.path.join(output_path, 'cv_max.txt'), projected_data_stats_dict['max'])\n",
    "np.savetxt(os.path.join(output_path, 'cv_min.txt'), projected_data_stats_dict['min'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
